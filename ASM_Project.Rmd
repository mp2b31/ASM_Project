---
title: "ASM-Project"
author: "Eloi Campeny Roig & Maria Paraskeva"
date: "2023-12-20"
output: html_document
editor_options: 
  chunk_output_type: inline
---

#Read Data
The chosen time series shows the apparent cement consumption in Spain (Construction Indicator) in thousands of tones.

Load the data and print them by month.
```{r}
serie=window(ts(read.table("cemento.dat"),start=1990,freq=12))
print(round(serie,0))
```

Plot the time series. 
```{r}
plot(serie,main="Apparent cement consumption in Spain (Thousand of tones)")
abline(v=1990:2020,col=4,lty=3)
```

# Transformations
Calculate the mean and variance per month in a matrix of 12 rows (months). Then plot the values of the annual mean against annual variance. 

```{r}
m=apply(matrix(serie,nr=12),2,mean)
v=apply(matrix(serie,nr=12),2,var)
plot(m,v,xlab="Annual Mean",ylab="Annual Variance",main="Annual Mean against Annual Variance values")
abline(lm(v~m),col=2,lty=3,lwd=2)
```

Plot the cement consumption over the years so as to gain insights into potential trends or seasonal patterns in construction activity.
Here we can clearly see that the height of the boxes (IQR) is higher for higher values of the mean, so we proceed in a change of scale. 

```{r}
boxplot(serie ~ floor(time(serie)), main = "Distribution of Time Series Data Across Time", xlab = "Year", ylab = "Cement Consumption")
```

Make a log transformation and plot the results. 
we chose the log transformation as it is easy to interpret. 

```{r}
lnserie=log(serie)
plot(lnserie, xlab = "Time", ylab = "Log Data", main = "Log transformation of Data over Time")
```

Calculate the log mean and log variance per month in a matrix of 12 rows (months).
Plot log variance against log mean.

```{r}
m=apply(matrix(lnserie,nr=12),2,mean)
v=apply(matrix(lnserie,nr=12),2,var)
plot(m,v,xlab="Annual Log Mean",ylab="Annual Log Variance",main="Annual Log Mean against Annual Log Variance values")
abline(lm(v~m),col=2,lty=3,lwd=2)
```

Plot the log cement consumption over discrete time intervals.

```{r}
boxplot(lnserie~floor(time(lnserie)), main = "Distribution of Log Data Across Time", xlab = "Year", ylab = "Log of Cement Consumption")
```

Plot the decomposition of additive time series. From top to bottom it shows:
 - Original time series
 - Trend Component
 - Seasonal Component
 - Residual Component

```{r}
plot(decompose(lnserie))
```

Seasonal plot of log data showing one subplot per month. The horizontal line in each subplot represents the mean value of the log-transformed time series. 

```{r}
monthplot(lnserie, main = "Seasonal plot of log data")
```

Plot the log-transformed data in a matrix format where each column is a different month. 
```{r}
ts.plot(matrix(lnserie,nrow=12),col=1:8, main = "Monthly Patterns in Log-Transformed Time Series Data")
```

Calculate the seasonal difference on the log data with a lag of 12. Then plot the results.

```{r}
d12lnserie=diff(lnserie,12)
plot(d12lnserie, main = "Seasonality difference in Log Series")
abline(h=0)
abline(h=mean(d12lnserie),col=2)
```

Add a differencing to the previous series with a lag of 1 and plot the results.

```{r}
d1d12lnserie=diff(d12lnserie,1)
plot(d1d12lnserie, main = "Regular difference in Seasonal Log Series")
abline(h=0)
abline(h=mean(d1d12lnserie),col=2,lwd=2)
```

Add another differencing with a lag of 1 and plot the results. 

```{r}
d1d1d12lnserie=diff(d1d12lnserie,1)
plot(d1d1d12lnserie, main = "Second Regular difference in Seasonal Log Series")
abline(h=0)
abline(h=mean(d1d12lnserie),col=2,lwd=2)
```

Show the variance of the transformed time series. The lower variance is that of the d1d12lnserie.

```{r}
var(lnserie)
var(d1d12lnserie)
var(d1d1d12lnserie) #make a table
```

# Simpler models

## Identification

Autocorrelation and partial autocorrelation function plots shown side by side for the differenced d1d12lnserie. 

```{r}
par(mfrow=c(1,2))
acf(d1d12lnserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=120, main = "ACF")
pacf(d1d12lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=120, main = "PACF")
```

The identified models are:
 - AR(2)
 - AR(4)
 - SAR(4)


## Estimation 

For the model AR(2) SAR(4) with zero regular and seasonal differences (d=0, D=0), we test to see if the intercept is significant.

```{r}
(mod1=arima(d1d12lnserie,order=c(2,0,0),seasonal=list(order=c(4,0,0),period=12)))
abs(mod1$coef/sqrt(diag(mod1$var.coef)))

intercept_t_ratio <- mod1$coef["intercept"] / sqrt(mod1$var.coef["intercept", "intercept"])

print(intercept_t_ratio)

```

Looking at the t-ratio we see that it is lower than 2 so the intercept is not significant. So we can use the model with the regular and seasonal differences (d=1, D=1)

```{r}
(mod1=arima(lnserie,order=c(2,1,0),seasonal=list(order=c(4,1,0),period=12)))
abs(mod1$coef/sqrt(diag(mod1$var.coef)))
```

The AIC is lower for the model without intercept.
All the weights are significant (all abs(t) values > 2).

We follow the same practices for the model AR(4) SAR(4) with zero regular and seasonal differences (d=0, D=0) to test if the intercept is significant.

```{r}
(mod2=arima(d1d12lnserie,order=c(4,0,0),seasonal=list(order=c(4,0,0),period=12)))
abs(mod2$coef/sqrt(diag(mod2$var.coef)))

intercept_t_ratio <- mod2$coef["intercept"] / sqrt(mod2$var.coef["intercept", "intercept"])

print(intercept_t_ratio)
```

Looking at the t-ratio we see that it is lower than 2 so the intercept is not significant. So we can use the model with the regular and seasonal differences (d=1, D=1).

```{r}
(mod2=arima(lnserie,order=c(4,1,0),seasonal=list(order=c(4,1,0),period=12)))
abs(mod2$coef/sqrt(diag(mod2$var.coef)))
```

The AIC is lower for the model with regular and seasonal difference. 
But the coef AR3 and AR4 are not significant enough.

```{r}
mod1$aic 
mod2$aic  
```

Comparing both models mod2 has lower AIC but the last two coefficients of AR do not pass the significance test. So we finally choose the first model AR(2) SAR(4) as it has almost the same AIC of model 2 but it is simpler and all coefficients are significant. 

## Validation

We proceed to the validation of our chosen model. 

```{r}
#################Validation#################################
validation=function(model,dades){ 
  s=frequency(get(model$series))
  resid=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resid,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resid)
  qqline(resid,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resid,breaks=20,freq=FALSE)
  curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #ACF & PACF of square residuals 
  par(mfrow=c(1,2))
  acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  suppressMessages(require(forecast,quietly=TRUE,warn.conflicts=FALSE))
  plot(model)
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:20])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:20])
  
  cat("\nDescriptive Statistics for the Residuals\n")
  cat("\n----------------------------------------\n") 
  
  suppressMessages(require(fBasics,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(basicStats(resid))
  
  ## Add here complementary tests (use with caution!)
  ##---------------------------------------------------------
  cat("\nNormality Tests\n")
  cat("\n--------------------\n")
 
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resid(model)))

  suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(ad.test(resid(model)))
  
  suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
  ##Jarque-Bera test
  print(jarque.bera.test(resid(model)))
  
  cat("\nHomoscedasticity Test\n")
  cat("\n--------------------\n")
  suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
  ##Breusch-Pagan test
  obs=get(model$series)
  print(bptest(resid(model)~I(obs-resid(model))))
  
  cat("\nIndependence Tests\n")
  cat("\n--------------------\n")
  
  ##Durbin-Watson test
  print(dwtest(resid(model)~I(1:length(resid(model)))))
  
  ##Ljung-Box test
  cat("\nLjung-Box test\n")
  print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
    te=Box.test(resid(model),type="Ljung-Box",lag=el)
    c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
  

  #Sample ACF vs. Teoric ACF
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  acf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample ACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1),
       type="h",xlab="Lag",  ylab="", main="ACF Teoric")
  abline(h=0)
  
  #Sample PACF vs. Teoric PACF
  pacf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample PACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
       type="h", xlab="Lag", ylab="", main="PACF Teoric")
  abline(h=0)
  par(mfrow=c(1,1))
}
################# Fi Validation #################################
```

```{r}
validation(mod1,d1d12lnserie)
```

The validation of model1 has these results:
- Residual analysis: The residuals appear to be random and centered around zero and there are some visible outliers. We could say that their variance is constant looking at the absolute residuals plot. There seems to be no clear pattern in the variability of the residuals over time, which suggests that the variance of the errors is constant (homoscedastic). The Q-Q plot shows some deviation in the tails, indicating that the residuals may not be perfectly normally distributed. The histogram shows that the residuals are roughly bell-shaped and centered around zero, even though there are more observations in the mean as well as the right side of the mean (heavy tail), indicating some slight skewness.

Looking at the statistical tests, we get a different interpretation as the normality tests (Shapiro-Wilk, Anderson-Darling, Jarque Bera) indicate that the residuals are not normally distributed (p-values are significant). Also, regarding homogeneity of variance, the Breusch Pagan test gives us ap-value of 0.531 suggests that there is no evidence of heteroscedasticity in the residuals.

- Autocorrelation: The Ljung-Box test checks for autocorrelation in residuals at different lags and some lags (like 12 and 48) show significant p-values, suggesting possible autocorrelation issues at these lags. However, the Durbin-Watson test suggests no autocorrelation in residuals (DW close to 2). 

The third plot that gives us another representation of the p-values for the Ljung-Box statistic over different lags indicates that some dots seem to be above the significance threshold (dashed lines), while most are below it. As the dots represent the p-value at each lag, we see that there is autocorrelation at the lags represented by the dots who are below the threshold. The Ljung-Box test fails after lag 9, so the samples are not independent.

- Invertibility: There is no number in the results so we cannot make an interpretation. As our model is only comprised of an AR component and has no MA component, the *theta* variable is empty. Hence, the concept of invertibility does not apply to our model.

-Causality: The model is stationary/causal as all the roots of the seasonal AR-characteristic polynomial lie outside of the unit circle witl all modul > 1. So, the model can be represented as a convergent MA(∞) expression with psi weights.


Summary: Outliers detection and its treatment should be the next step, and possibly, better validation results can be obtained. Also calendar effects analysis should be performed. 

Below we continue with a comparison of one model until December 2018 and the complete model, in order to see if the model generalises well with the data.

```{r}
ultim=c(2018,12)
pdq=c(2,1,0) 
PDQ=c(4,1,0)

serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)

(mod1=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))

(mod2=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
```

both models are similar, more or less the same coef

```{r}
pred=predict(mod2,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)

reduced <- window(serie,start=ultim-c(4,0) ,end=ultim+c(1,0))

top <- max(reduced,tu)
bot <- min(reduced,tl)
mid <- top - bot

ts.plot(reduced,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),ylim=c(bot-0.1*mid,top+0.1*mid),type="o",main="Model ARIMA(2,1,0)(4,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))
```
Errors of the first model

```{r}
obs=window(serie,start=ultim)
mod.RMSE1=sqrt(sum((obs-pr)^2)/12)
mod.MAE1=sum(abs(obs-pr))/12
mod.RMSPE1=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE1=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE1,"MAE"=mod.MAE1,"RMSPE"=mod.RMSPE1,"MAPE"=mod.MAPE1)
```

```{r}
mCI1=mean(tu-tl)

cat("\nMean Length CI: ",mCI1)
```

```{r}
pred=predict(mod1,n.ahead=12)
pr<-ts(c(tail(lnserie1,1),pred$pred),start=ultim+c(1,0),freq=12)

se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr<-ts(exp(pr),start=ultim+c(1,0),freq=12)

reduced <- window(serie,start=ultim-c(3,0) ,end=ultim+c(1,0))

top <- max(reduced,tu)
bot <- min(reduced,tl)
mid <- top - bot

ts.plot(reduced,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,+3),ylim=c(bot-0.1*mid,top+0.1*mid),type="o",main="Model ARIMA(2,1,0)(4,1,0)12")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)

```


# Calendar Effect

```{r}
source("CalendarEffects.r")
```

```{r}
data=c(start(lnserie)[1],start(lnserie)[2], length(lnserie))

(wTradDays=Wtrad(data))
```

```{r}
(wEast=Weaster(data))
```

Model with Trading Days

```{r}
(modTD=arima(lnserie,order=pdq,seasonal=list(order=PDQ,period=12),xreg=wTradDays))
abs(modTD$coef/sqrt(diag(modTD$var.coef)))
```
Model with Easter week
```{r}
(modEa=arima(lnserie,order=pdq,seasonal=list(order=PDQ,period=12),xreg=wEast))
abs(modEa$coef/sqrt(diag(modEa$var.coef)))
```
Model with Trading days and Easter weeks

```{r}
(modEC=arima(lnserie,order=pdq,seasonal=list(order=PDQ,period=12),xreg=data.frame(wTradDays,wEast)))
abs(modEC$coef/sqrt(diag(modEC$var.coef)))
```

```{r}
mod1$aic
modTD$aic
modEa$aic
modEC$aic

#make a table with the results
```

best model modEC

```{r}
EfecTD=coef(modEC)["wTradDays"]*wTradDays
EfecSS=coef(modEC)["wEast"]*wEast
lnserieEC=lnserie-EfecTD-EfecSS

d1d12lnserieEC=diff(diff(lnserieEC,12))
```

## Identification

Identification of the models without calendar effects. 

```{r}
par(mfrow=c(1,2))
acf(d1d12lnserieEC,ylim=c(-1,1),lag.max=120,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12lnserieEC,ylim=c(-1,1),lag.max=120,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```

MA(1), AR(2), SMA(1)

## Estimation

```{r}
(mod1EC=arima(lnserie,order=c(1,1,0),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod1EC$coef/sqrt(diag(mod1EC$var.coef)))
```

```{r}
(mod2EC=arima(lnserie,order=c(0,1,2),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod2EC$coef/sqrt(diag(mod2EC$var.coef)))
```

mod2EC best model, lower AIC

```{r}
pdq <- c(0,1,2)
PDQ <- c(1,1,0)
```


```{r}
EfecTD=coef(mod2EC)["wTradDays"]*wTradDays
EfecSS=coef(mod2EC)["wEast"]*wEast
plot(EfecTD+EfecSS, main = "Calendar Effects")
```

```{r}
reduced <- window(serie,start=ultim-c(29,0) ,end=ultim+c(1,0))

lnserieEC=lnserie-EfecTD-EfecSS

serieEC=exp(lnserieEC)
plot(reduced,type="o", main = "Comparison between Original Series \nand Series without Calendar Effects")
lines(serieEC,col=2,type="o")
abline(v=1990:2020,lty=3,col=4)
```

```{r}
reduced <- window(serie,start=ultim-c(4,0) ,end=ultim+c(1,0))

lnserieEC=lnserie-EfecTD-EfecSS
serieEC=exp(lnserieEC)
reducedEC <- window(serieEC,start=ultim-c(4,0) ,end=ultim+c(1,0))

plot(reduced,type="o", main = "Comparison between Original Series \nand Series without Calendar Effects \nin the last five years")
lines(reducedEC,col=2,type="o")
abline(v=1990:2020,lty=3,col=4)

```

```{r}
window(cbind(wTradDays,wEast,serie,serieEC),start=2014)
```

## Validation

```{r}
validation(mod2EC,d1d12lnserieEC)
```

```{r}
ultim=c(2018,12)

serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)
wTradDays2=window(wTradDays,end=ultim)
wEast2=window(wEast,end=ultim)

(modEC=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12),xreg=data.frame(wTradDays,wEast)))

(modEC2=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12),xreg=data.frame(wTradDays2,wEast2)))
```
Predictions

```{r}
pred=predict(modEC2,n.ahead=12,newxreg=window(cbind(wTradDays,wEast),start=c(ultim[1]+1,1)))
predic=pred$pr

pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)

reduced <- window(serie,start=ultim-c(4,0) ,end=ultim+c(1,0))

top <- max(reduced,tu)
bot <- min(reduced,tl)
mid <- top - bot

ts.plot(reduced,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),ylim=c(bot-0.1*mid,top+0.1*mid),type="o",main="Model ARIMA(0,1,2)(1,1,0)12+TD+Easter")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))
```
Error Comparison
```{r}
obs=window(serie,start=ultim)
mod.RMSE1=sqrt(sum((obs-pr)^2)/12)
mod.MAE1=sum(abs(obs-pr))/12
mod.RMSPE1=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE1=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE1,"MAE"=mod.MAE1,"RMSPE"=mod.RMSPE1,"MAPE"=mod.MAPE1)
```

```{r}
mCI1=mean(tu-tl)

cat("\nMean Length CI: ",mCI1)
```

# Outlier treatment

We now proceed to take a look at the outliers, first by identifying them and then by treating them.

```{r}
########## Atípics (Outliers) ###############################################
source("atipics2.r")
```

## Outlier identification

```{r}

modEC.atip=outdetec(mod2EC,dif=c(1,12),crit=3,LS=T)
modEC.atip$sigma
```

```{r}
atipics=modEC.atip$atip[order(modEC.atip$atip[,1]),]
month=c("Ene","Feb","Mar","Abr","May","Jun","Jul","Ago","Sep","Oct","Nov","Dic")

data.frame(atipics,Fecha=paste(month[(atipics[,1]-1)%%12+1],start(lnserie)[1]+((atipics[,1]-1)%/%12)),perc.Obs=exp(atipics[,3])*100)
# make a table with these results
```

```{r}
lnserie.lin=lineal(lnserie,modEC.atip$atip)
serie.lin=exp(lnserie.lin)
```

```{r}
lnserieEC.lin=lineal(lnserieEC,modEC.atip$atip)
serieEC.lin=exp(lnserieEC.lin)
```


## Model identification

```{r}
d1d12lnserieEC.lin=diff(diff(lnserieEC.lin,12))
par(mfrow=c(1,2))
acf(d1d12lnserieEC.lin,ylim=c(-1,1),lag.max=120,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12lnserieEC.lin,ylim=c(-1,1),lag.max=120,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```
The models we hava identified from the ACF and PACF plots are the following:
AR(2), AR(4), MA(2), MA(4), SAR(1), SMA(1), SMA(3)

## Estimation

```{r}
(mod1EC.lin=arima(lnserie.lin,order=c(2,1,0),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod1EC.lin$coef/sqrt(diag(mod1EC.lin$var.coef)))
```

```{r}
(mod2EC.lin=arima(lnserie.lin,order=c(4,1,0),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod2EC.lin$coef/sqrt(diag(mod2EC.lin$var.coef)))
```

```{r}
(mod3EC.lin=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod3EC.lin$coef/sqrt(diag(mod3EC.lin$var.coef)))
```

```{r}
(mod4EC.lin=arima(lnserie.lin,order=c(0,1,4),seasonal=list(order=c(1,1,0),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod4EC.lin$coef/sqrt(diag(mod4EC.lin$var.coef)))
```

not significant ma3,ma4

```{r}
(mod5EC.lin=arima(lnserie.lin,order=c(2,1,0),seasonal=list(order=c(0,1,1),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod5EC.lin$coef/sqrt(diag(mod5EC.lin$var.coef)))
```

```{r}
(mod6EC.lin=arima(lnserie.lin,order=c(4,1,0),seasonal=list(order=c(0,1,1),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod6EC.lin$coef/sqrt(diag(mod6EC.lin$var.coef)))
```

```{r}
(mod7EC.lin=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(0,1,1),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod7EC.lin$coef/sqrt(diag(mod7EC.lin$var.coef)))
```

```{r}
(mod8EC.lin=arima(lnserie.lin,order=c(0,1,4),seasonal=list(order=c(0,1,1),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod8EC.lin$coef/sqrt(diag(mod8EC.lin$var.coef)))
```

not significant ma3,ma4

```{r}
(mod9EC.lin=arima(lnserie.lin,order=c(2,1,0),seasonal=list(order=c(0,1,3),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod9EC.lin$coef/sqrt(diag(mod9EC.lin$var.coef)))
```

```{r}
(mod10EC.lin=arima(lnserie.lin,order=c(4,1,0),seasonal=list(order=c(0,1,3),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod10EC.lin$coef/sqrt(diag(mod10EC.lin$var.coef)))
```

```{r}
(mod11EC.lin=arima(lnserie.lin,order=c(0,1,2),seasonal=list(order=c(0,1,3),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod11EC.lin$coef/sqrt(diag(mod11EC.lin$var.coef)))
```

```{r}
(mod12EC.lin=arima(lnserie.lin,order=c(0,1,4),seasonal=list(order=c(0,1,3),period=12),xreg=data.frame(wTradDays,wEast)))
abs(mod12EC.lin$coef/sqrt(diag(mod12EC.lin$var.coef)))
```
After creating twelve different models, we compare their AIC index in order to find the best one. As shown below, the model with the lowest AIC is model 9.

```{r}
mod1EC.lin$aic
mod2EC.lin$aic
mod3EC.lin$aic
mod4EC.lin$aic

mod5EC.lin$aic
mod6EC.lin$aic
mod7EC.lin$aic
mod8EC.lin$aic

mod9EC.lin$aic
mod10EC.lin$aic
mod11EC.lin$aic
mod12EC.lin$aic

# create table with results
```


The model 9 has these parameters below:

```{r}
pdq <- c(2,1,0)
PDQ <- c(0,1,3)
```


```{r}
EfecTD=coef(mod9EC.lin)["wTradDays"]*wTradDays
EfecSS=coef(mod9EC.lin)["wEast"]*wEast
lnserieEC.lin=lnserie.lin-EfecTD-EfecSS
plot(lnserie-lnserieEC.lin, main = "Outlier Effects")

plot(exp(lnserieEC.lin),col=2, main = "Series Comparison Original vs. without Calendar Effects and Outliers")
lines(serie)
```

```{r}
d12lnserieEC.lin=diff(lnserieEC.lin,12)
d1d12lnserieEC.lin=diff(d12lnserieEC.lin,1)

```

## Validation

We proceed to validate model 9.



```{r}
validation(mod9EC.lin,d1d12lnserieEC.lin)
```

```{r}
ultim=c(2018,12)

serie.lin1=window(serie.lin,end=ultim+c(1,0))
lnserie.lin1=log(serie.lin1)
serie.lin2=window(serie.lin,end=ultim)
lnserie.lin2=log(serie.lin2)
wTradDays2=window(wTradDays,end=ultim)
wEast2=window(wEast,end=ultim)
```

```{r}
(modEC.lin=arima(lnserie.lin1,order=pdq,seasonal=list(order=PDQ,period=12),xreg=data.frame(wTradDays,wEast)))

(modEC.lin2=arima(lnserie.lin2,order=pdq,seasonal=list(order=PDQ,period=12),xreg=data.frame(wTradDays2,wEast2)))

```
Prediction for the year 2020.

```{r}
pred=predict(modEC.lin2,n.ahead=12,newxreg=window(cbind(wTradDays,wEast),start=c(ultim[1]+1,1)))
wLS=sum(modEC.atip$atip[modEC.atip$atip$type_detected=="LS" & modEC.atip$atip$Obs<=length(serie)-12,3])
predic=pred$pr+wLS
pr<-ts(c(tail(lnserie2,1),predic),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)

ts.plot(reduced,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,2),type="o",main="Model ARIMA(2,1,0)(0,1,3)12+TD+Easter+Outliers")
abline(v=(ultim[1]-3):(ultim[1]+3),lty=3,col=4)

```

```{r}
EfecTD=coef(mod9EC.lin)["wTradDays"]*wTradDays
EfecSS=coef(mod9EC.lin)["wEast"]*wEast
lnserieEC.lin=lnserie.lin-EfecTD-EfecSS
plot(lnserie-lnserieEC.lin)
```

```{r}
(previs.lin=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))
```
Error Comparison.

```{r}
obs=window(serie,start=ultim)
mod.RMSE3=sqrt(sum((obs-pr)^2)/12)
mod.MAE3=sum(abs(obs-pr))/12
mod.RMSPE3=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE3=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE3,"MAE"=mod.MAE3,"RMSPE"=mod.RMSPE3,"MAPE"=mod.MAPE3)
```

```{r}
mCI3=mean(tu-tl)

cat("\nMean Length CI: ",mCI3)
```

# Long term predictions with complete model

```{r}
data3=c(ultim[1]+2, 1, 12)

wTradDays3=Wtrad(data3)
wEast3=Weaster(data3)
pred=predict(modEC.lin,n.ahead=12,newxreg=data.frame(wTradDays3,wEast3))
wLS=sum(modEC.atip$atip[modEC.atip$atip$type_detected=="LS",3])
predic=pred$pr + wLS

pr<-ts(c(lnserie[length(lnserie)],predic),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl3<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu3<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr3<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl3,tu3,pr3,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,3),ylim=c(500,2000),type="o",main="Model ARIMA(2,1,0)(0,1,3)12+TD+Easter+Outliers")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```
Long term predictions table

```{r}
(previs=window(cbind(tl3,pr3,tu3),start=ultim+c(1,0)))
```

